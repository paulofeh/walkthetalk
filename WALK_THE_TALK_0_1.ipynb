{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6EtnXL1EiCF"
      },
      "source": [
        "# Walk the Talk (Monitor ESG)\n",
        "v. 0.1 - Foco em Carbono"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y35hl7Rkrcly"
      },
      "source": [
        "## Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJKo17jvJZno",
        "outputId": "e4981ddf-f0dc-48e8-c080-c0ee345d8bd5"
      },
      "outputs": [],
      "source": [
        "# Instala as bibliotecas necessárias\n",
        "! pip install PyPDF2\n",
        "! pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVDlFfKCJeX2",
        "outputId": "92cfee4f-a37c-48d3-b289-3076ae6be08b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n"
          ]
        }
      ],
      "source": [
        "# Importa as bibliotecas e módulos necessários\n",
        "import PyPDF2\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import requests\n",
        "import spacy\n",
        "spacy.cli.download(\"pt_core_news_lg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJJDu9-qJmyx"
      },
      "source": [
        "## Funções"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXj6gaVSiUhz"
      },
      "source": [
        "### THE TALK\n",
        "Raspagem de fontes internas (relatórios ESG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qn1fQHeql3c"
      },
      "source": [
        "#### Análise do relatório"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "jAtPVJauqqmq"
      },
      "outputs": [],
      "source": [
        "def analisa_relatorio(empresa, url, termos):\n",
        "\n",
        "  # Etapa 1: requisita o PDF e salva em uma variável\n",
        "\n",
        "  # Truque para evitar que o servidor bloqueie o acesso\n",
        "  headers = {\n",
        "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0'\n",
        "  }\n",
        "\n",
        "  response = requests.get(caminho_pdf, headers=headers)\n",
        "\n",
        "  if response.status_code != 200:\n",
        "    print(f'Erro! A requisição à URL retornou o status {response.status_code}')\n",
        "\n",
        "  else:\n",
        "\n",
        "    # Salva o arquivo baixado\n",
        "    with open(f'{empresa}.pdf', 'wb') as pdf_file:\n",
        "        pdf_file.write(response.content)\n",
        "        print(f\"O relatório da empresa {empresa} foi baixado com sucesso.\")\n",
        "\n",
        "    # Reabre o arquivo no modo de leitura binária para uso com PyPDF2\n",
        "    with open(f'{empresa}.pdf', 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "  # Etapa 2: conversão do PDF em string + limpeza do texto\n",
        "\n",
        "        pdf_to_string = ''\n",
        "        for x in range(0, len(pdf_reader.pages)):\n",
        "            pagina = pdf_reader.pages[x]\n",
        "            texto_pagina = pagina.extract_text()\n",
        "            pdf_to_string = pdf_to_string + texto_pagina\n",
        "        print(f\"O relatório foi convertido em string.\")\n",
        "\n",
        "    # Lista de substituições a serem feitas\n",
        "    substituicoes = [\n",
        "    (r'-\\n', ''),  # Substitui hífen seguido por quebra de linha\n",
        "    (r'\\n', ' ') # Substitui todas as quebras de linha por espaço\n",
        "    # Adicionar outras se necessário\n",
        "    ]\n",
        "\n",
        "    # Realiza substituições\n",
        "    for padrao, substituicao in substituicoes:\n",
        "        texto = re.sub(padrao, substituicao, pdf_to_string)\n",
        "\n",
        "    # Remove espaços em branco extra\n",
        "    texto = ' '.join(texto.split())\n",
        "\n",
        "    # Etapa 3: processamento do texto com NLP para obtenção de frases\n",
        "\n",
        "    # Carrega o modelo NLP em português\n",
        "    nlp = spacy.load(\"pt_core_news_lg\")\n",
        "    doc = nlp(texto)\n",
        "\n",
        "    # Filtra frases que contêm pelo menos um dos termos de interesse\n",
        "    frases = [sent.text for sent in doc.sents if any(termo in sent.text for termo in termos)]\n",
        "\n",
        "    # Etapa 4: criação e inserção de dados no dataframe\n",
        "\n",
        "    df = pd.DataFrame(columns=['empresa', 'frase'])\n",
        "    for frase in frases:\n",
        "      df.loc[len(df.index)] = [empresa, frase]\n",
        "\n",
        "    print(f'Foram selecionadas {len(df)} frases do relatório da empresa {empresa}.')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PS2NJk4iBrX"
      },
      "source": [
        "### THE WALK\n",
        "Raspagem de fontes externas:\n",
        "- Bloomberg Línea Brasil (API oculta)\n",
        "- Folha de S. Paulo (HTML)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DALuDmIc7-jv"
      },
      "source": [
        "#### Raspagem Bloomberg Línea\n",
        "- O robô acessa as notícias mais recentes do canal ESG do portal e procura pelo nome da empresa e termos de interesse indicados\n",
        "- O número de registros pode ser indicado como parâmetro na função"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h1eDbQXRbA85"
      },
      "outputs": [],
      "source": [
        "# ROBÔ BLOOMBERG LINEA (ARTIGOS)\n",
        "\n",
        "# Os parâmetros passados são o nome da empresa a ser pesquisada (string), os termos de interesse (lista) e o número de artigos a serem raspados (int)\n",
        "# A função retorna um dataframe com 5 colunas: data do artigo, título do artigo, nome do veículo, url e conteúdo dos artigos contendo o nome da empresa e os termos de interesse\n",
        "\n",
        "def raspa_bloomberg(empresa, termos, num_artigos):\n",
        "\n",
        "  # Cria um dataframe Pandas com 5 colunas\n",
        "  df = pd.DataFrame(columns=['Titulo', 'Veiculo', 'URL', 'Artigo', 'Data_Publicacao'])\n",
        "\n",
        "  nome_veiculo = 'Bloomberg Línea'\n",
        "\n",
        "  # ETAPA 1: Na página do tópico ESG, encontrar os links para as matérias mais recentes de acordo com num_artigos\n",
        "\n",
        "  # cabeçalhos da busca pela API oculta\n",
        "  headers = {\n",
        "    'authority': 'www.bloomberglinea.com.br',\n",
        "    'accept': '*/*',\n",
        "    'accept-language': 'pt,en-US;q=0.9,en;q=0.8,es;q=0.7',\n",
        "    'cache-control': 'no-cache',\n",
        "    'dnt': '1',\n",
        "    'if-modified-since': '1700071058771',\n",
        "    'pragma': 'no-cache',\n",
        "    'referer': 'https://www.bloomberglinea.com.br/esg/',\n",
        "    'sec-ch-ua': '\"Microsoft Edge\";v=\"119\", \"Chromium\";v=\"119\", \"Not?A_Brand\";v=\"24\"',\n",
        "    'sec-ch-ua-mobile': '?0',\n",
        "    'sec-ch-ua-platform': '\"Windows\"',\n",
        "    'sec-fetch-dest': 'empty',\n",
        "    'sec-fetch-mode': 'cors',\n",
        "    'sec-fetch-site': 'same-origin',\n",
        "    'sec-gpc': '1',\n",
        "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0',\n",
        "  }\n",
        "\n",
        "  # parâmetros da busca pela API oculta\n",
        "  # o valor de 'feedSize' pode ser customizado para num_artigos\n",
        "  params = {\n",
        "    'query': '{\"excludeSections\":\"/videos\",\"feature\":\"T1-Small-Small-Small-Small\",\"feedOffset\":0,\"feedSize\":' + str(num_artigos) + ',\"includeSections\":\"/esg\"}',\n",
        "    'filter': '{content_elements{description{basic},display_date,headlines{basic},last_updated_date,taxonomy{primary_section{name,path}},websites{bloomberg-linea-brasil{website_url}}}}',\n",
        "    'd': '1525',\n",
        "    '_website': 'bloomberg-linea-brasil',\n",
        "  }\n",
        "\n",
        "  # requisita o endpoint da API\n",
        "  response = requests.get(\n",
        "      'https://www.bloomberglinea.com.br/pf/api/v3/content/fetch/story-feed-sections',\n",
        "      params=params,\n",
        "      headers=headers,\n",
        "  )\n",
        "\n",
        "  # Verificar se a requisição foi bem-sucedida (código de status 200)\n",
        "  if response.status_code == 200:\n",
        "\n",
        "    print('Bloomberg Línea visitado com sucesso.')\n",
        "\n",
        "    # interpreta como json\n",
        "    bloomberg_esg = response.json()['content_elements']\n",
        "\n",
        "    # cria uma lista para receber as URLs de cada matéria dentro do tópico\n",
        "    links_bloomberg = []\n",
        "\n",
        "    # adiciona cada URL encontrada à lista\n",
        "    for item in bloomberg_esg:\n",
        "\n",
        "      # Acessar o valor de 'website_url'\n",
        "      website_url = item.get('websites', {}).get('bloomberg-linea-brasil', {}).get('website_url')\n",
        "\n",
        "      # Adiciona o valor de 'website_url' à lista\n",
        "      links_bloomberg.append(f'https://bloomberglinea.com.br{website_url}')\n",
        "\n",
        "  else:\n",
        "    print(f'ERRO. A requisição a Bloomberg Linea retornou o status {response.status_code}.')\n",
        "\n",
        "  # ETAPA 2: Visita cada link encontrado e procura pelo nome da empresa\n",
        "\n",
        "  for url in links_bloomberg:\n",
        "\n",
        "    # Requisita a página via requests\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Verifica se a requisição foi bem-sucedida (código de status 200)\n",
        "    if response.status_code == 200:\n",
        "\n",
        "      # Informa o status da requisição\n",
        "      print(f'{url} OK.')\n",
        "\n",
        "      # Cria um objeto BeautifulSoup com o conteúdo HTML da página\n",
        "      soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "      # Encontra o título principal\n",
        "      titulo = soup.find('h1').text\n",
        "\n",
        "      # Encontra a data de publicação\n",
        "      data = soup.find('div', {'class' : 'mt-2 mb-0 lg:mb-1'}).find('small').text\n",
        "\n",
        "      # Encontra todos os parágrafos\n",
        "      paragrafos = soup.find_all('p')\n",
        "\n",
        "      # Verificar se o nome da empresa está presente em pelo menos um parágrafo\n",
        "      empresa_encontrada = any(empresa in paragrafo.get_text() for paragrafo in paragrafos)\n",
        "\n",
        "      # Se o nome da empresa encontrado, adiciona ao dataframe os parágrafos que contêm pelo menos um dos termos de interesse\n",
        "      if empresa_encontrada:\n",
        "        print(f'O nome da empresa {empresa} foi encontrado neste texto.')\n",
        "\n",
        "        # Verifica se pelo menos um dos termos de interesse está presente em algum parágrafo\n",
        "        termo_encontrado = any(any(termo.lower() in paragrafo.get_text().lower() for termo in termos) for paragrafo in paragrafos)\n",
        "\n",
        "        # Se o termo for encontrado, salva no dataframe\n",
        "        if termo_encontrado:\n",
        "          print(f'Um termo de interesse foi encontrado neste texto.')\n",
        "          txt = ''\n",
        "          for paragrafo in paragrafos:\n",
        "            txt = txt + paragrafo.text\n",
        "          df.loc[len(df.index)] = [titulo, nome_veiculo, url, txt, data]\n",
        "        else: print(f'Nenhum dos termos de interesse foi encontrado neste texto.')\n",
        "      else:\n",
        "        print(f'O nome da empresa {empresa} não foi encontrado neste texto.')\n",
        "    else:\n",
        "      print(f'ERRO. A requisição a {url} retornou o status {response.status_code}.')\n",
        "\n",
        "    time.sleep(2) # dorme por 2 segundos\n",
        "\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Raspagem Folha\n",
        "- O robô realiza uma busca pelo nome da empresa e termos de interesse nas notícias do último ano"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "def raspa_folha(empresa, termos):\n",
        "\n",
        "    # Cria um dataframe Pandas com 5 colunas\n",
        "    df = pd.DataFrame(columns=['Titulo', 'Veiculo', 'URL', 'Artigo', 'Data_Publicacao'])\n",
        "\n",
        "    nome_veiculo = 'Folha de S. Paulo'\n",
        "    links_folha = []\n",
        "\n",
        "    # ETAPA 1: Busca pelas matérias que contenham tanto o nome da empresa quanto os termos de interesse\n",
        "\n",
        "    for termo in termos:\n",
        "\n",
        "        # Monta a URL de busca a partir dos parâmetros\n",
        "        # Substitua todos os espaços em branco por '+'\n",
        "        busca = termo.replace(\" \", \"+\")\n",
        "\n",
        "        url = f'https://search.folha.uol.com.br/search?q={empresa}+{busca}&periodo=ano&sd=&ed=&site=todos'\n",
        "        print(url)\n",
        "\n",
        "        # requisita a primeira url de busca\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            print(f'{nome_veiculo} visitado com sucesso.')\n",
        "            sopa = BeautifulSoup(response.text, 'html.parser')\n",
        "            resultados = sopa.find('div', {'class' : 'c-search__result'}).get_text()\n",
        "            \n",
        "            # Encontre todos os números na string\n",
        "            num_resultados = re.findall('\\d+', resultados)\n",
        "\n",
        "            # O resultado é uma lista de strings, então precisamos converter o primeiro elemento para int\n",
        "            if len(num_resultados) > 0:\n",
        "                num_resultados = int(num_resultados[0])\n",
        "\n",
        "                # Se houver resultados, calcula o número de páginas de resultados (cada página tem 25 resultados)\n",
        "                num_paginas = num_resultados // 25 + 1\n",
        "                print(f'A busca por {empresa} e {termo} retornou {num_resultados} artigos em {num_paginas} páginas.')\n",
        "\n",
        "                # Cria uma lista para receber as URLs de cada matéria na primeira página de busca (ou única página, se for o caso)\n",
        "                links = sopa.find_all('div', {'class' : 'c-headline__content'})\n",
        "                links = [link.find('a').get('href') for link in links if '#' not in link.find('a').get('href')]\n",
        "                print(f'Página 1 de {num_paginas} visitada, com {len(links)} links adicionados.')\n",
        "\n",
        "                # Se houver apenas uma página de resultados, adiciona os links à lista\n",
        "                if num_paginas == 1:\n",
        "                    links_folha = links_folha + links            \n",
        "\n",
        "                # Se houver mais de uma página de resultados, adiciona os links à lista e visita as páginas restantes\n",
        "                else:\n",
        "                    links_folha = links_folha + links\n",
        "                    pagina = 0\n",
        "                    while pagina < (num_paginas -1):\n",
        "                        pagina = pagina + 1\n",
        "                        print(f'Página {pagina + 1} de {num_paginas} visitada.')\n",
        "                        url = f'https://search.folha.uol.com.br/search?q={empresa}+{busca}&periodo=ano&sd=&ed=&site=todos&results_count={num_resultados}&sr={pagina * 25 + 1}'\n",
        "                        print(url)\n",
        "                        response = requests.get(url)\n",
        "                        sopa = BeautifulSoup(response.text, 'html.parser')\n",
        "                        links = []\n",
        "                        links = sopa.find_all('div', {'class' : 'c-headline__content'})\n",
        "                        links_folha.extend([link.find('a').get('href') for link in links if link.find('a').get('href') not in links_folha and '#' not in link.find('a').get('href')])\n",
        "            else:\n",
        "                print(f'A busca por {empresa} e {termo} não retornou resultados.')\n",
        "\n",
        "    len(links_folha)\n",
        "\n",
        "    # ETAPA 2: Visita cada link encontrado e salva o conteúdo dos artigos que contêm o nome da empresa e os termos de interesse\n",
        "\n",
        "    df = pd.DataFrame(columns=['Titulo', 'Veiculo', 'URL', 'Artigo', 'Data_Publicacao'])\n",
        "\n",
        "    for link in links_folha:\n",
        "        \n",
        "            # Requisita a página via requests\n",
        "            response = requests.get(link)\n",
        "        \n",
        "            # Verifica se a requisição foi bem-sucedida (código de status 200)\n",
        "            if response.status_code == 200:\n",
        "        \n",
        "                # Informa o status da requisição\n",
        "                print(f'{link} OK.')\n",
        "        \n",
        "                # Decodifica o conteúdo da resposta usando 'utf-8'\n",
        "                content = response.content #.decode('utf-8')\n",
        "\n",
        "                # Cria um objeto BeautifulSoup com o conteúdo HTML da página\n",
        "                soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "                # Encontra o título principal\n",
        "                titulo = soup.find('h1', class_='c-content-head__title').get_text().strip()\n",
        "        \n",
        "                # Encontra a data de publicação\n",
        "\n",
        "                # Find the 'time' element with the specified class\n",
        "                time_element = soup.find('time', class_='c-more-options__published-date')\n",
        "\n",
        "                # Get the value of 'datetime' attribute\n",
        "                data = time_element['datetime']\n",
        "                    \n",
        "                # Encontra todos os parágrafos\n",
        "                paragrafos = soup.find('div', class_='c-news__body').find_all('p')\n",
        "\n",
        "                # Extrai o texto de cada parágrafo e junta tudo em uma única string\n",
        "                texto = ' '.join(paragrafo.get_text().strip() for paragrafo in paragrafos) \n",
        "\n",
        "                df.loc[len(df.index)] = [titulo, nome_veiculo, link, texto, data]\n",
        "                \n",
        "            else:\n",
        "                print(f'ERRO. A requisição a {link} retornou o status {response.status_code}.')\n",
        "        \n",
        "            time.sleep(3) # dorme por 3 segundos\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVYVt3Pvtmat"
      },
      "source": [
        "## Aperta o play\n",
        "\n",
        "- Os termos de interesse podem ser adicionados à lista conforme desejado\n",
        "- URLs de relatórios ESG já testadas:\n",
        "    - Petrobras: https://sustentabilidade.petrobras.com.br/documents/1449993/80c5cf69-cb78-5ae4-aa27-46f958da64ba "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3g_lUltKXth"
      },
      "source": [
        "#### Variáveis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "EHjt5Kf5KXWn"
      },
      "outputs": [],
      "source": [
        "# Lista com os termos de interesse\n",
        "termos = ['carbono', 'co2', 'descarbonização', 'net zero']\n",
        "\n",
        "# Caminho do PDF a ser analisado\n",
        "caminho_pdf = 'https://sustentabilidade.petrobras.com.br/documents/1449993/80c5cf69-cb78-5ae4-aa27-46f958da64ba'\n",
        "\n",
        "# Nome da empresa a ser pesquisada no noticiário (em minúsculas)\n",
        "empresa = 'Petrobras'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ON6DXMr-vJa"
      },
      "source": [
        "### Execução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deNFnz5UFRWl",
        "outputId": "64337cfd-312d-4948-f37d-00f647152c00"
      },
      "outputs": [],
      "source": [
        "the_talk = analisa_relatorio(empresa, caminho_pdf, termos)\n",
        "\n",
        "the_walk = pd.concat([raspa_bloomberg(empresa, termos, 10), raspa_folha(empresa, termos)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "Q3ojndP-wi5I",
        "outputId": "7f45ff14-bb49-4be7-ce88-91f123ce0aad"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Titulo</th>\n",
              "      <th>Veiculo</th>\n",
              "      <th>URL</th>\n",
              "      <th>Artigo</th>\n",
              "      <th>Data_Publicacao</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Líderes vão à COP28 sob calor recorde e com em...</td>\n",
              "      <td>Bloomberg Línea</td>\n",
              "      <td>https://bloomberglinea.com.br/esg/lideres-vao-...</td>\n",
              "      <td>Expectativa é de que encontro da ONU resulte e...</td>\n",
              "      <td>29 de Novembro, 2023 | 03:37 PM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Na COP28, Brasil e mais 115 países preveem tri...</td>\n",
              "      <td>Folha de S. Paulo</td>\n",
              "      <td>https://www1.folha.uol.com.br/ambiente/2023/12...</td>\n",
              "      <td>O presidente da COP28, conferência do clima da...</td>\n",
              "      <td>2023-12-02 19:25:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Petrobras aprova retomada de obras de refinari...</td>\n",
              "      <td>Folha de S. Paulo</td>\n",
              "      <td>https://www1.folha.uol.com.br/mercado/2023/11/...</td>\n",
              "      <td>A Petrobras aprovou em seu novo plano estratég...</td>\n",
              "      <td>2023-11-27 20:55:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Agenda verde entra no radar da Câmara às véspe...</td>\n",
              "      <td>Folha de S. Paulo</td>\n",
              "      <td>https://www1.folha.uol.com.br/mercado/2023/11/...</td>\n",
              "      <td>O presidente da Câmara dos Deputados, Arthur L...</td>\n",
              "      <td>2023-11-27 08:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Quente, fervendo</td>\n",
              "      <td>Folha de S. Paulo</td>\n",
              "      <td>https://www1.folha.uol.com.br/opiniao/2023/11/...</td>\n",
              "      <td>No Sul, chuvas torrenciais assolam cidades e l...</td>\n",
              "      <td>2023-11-25 22:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>Mundo não deixará de precisar de petróleo, diz...</td>\n",
              "      <td>Folha de S. Paulo</td>\n",
              "      <td>https://www1.folha.uol.com.br/ambiente/2023/05...</td>\n",
              "      <td>Primeiro diretor da Petrobras voltado para tra...</td>\n",
              "      <td>2023-05-31 11:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>De carro popular à Foz do Amazonas, projetos c...</td>\n",
              "      <td>Folha de S. Paulo</td>\n",
              "      <td>https://www1.folha.uol.com.br/ambiente/2023/05...</td>\n",
              "      <td>A discussão em torno da exploração de petróleo...</td>\n",
              "      <td>2023-05-29 19:51:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>Térmicas a carvão no Sul pesam nas emissões de...</td>\n",
              "      <td>Folha de S. Paulo</td>\n",
              "      <td>https://www1.folha.uol.com.br/ambiente/2023/10...</td>\n",
              "      <td>As térmicas a carvão foram os destaques em emi...</td>\n",
              "      <td>2023-10-19 12:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>Semântica petroleira</td>\n",
              "      <td>Folha de S. Paulo</td>\n",
              "      <td>https://www1.folha.uol.com.br/opiniao/2023/03/...</td>\n",
              "      <td>Imagine se em fevereiro do ano passado Vladimi...</td>\n",
              "      <td>2023-03-09 21:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>Petrobras tem compromisso com a transição ener...</td>\n",
              "      <td>Folha de S. Paulo</td>\n",
              "      <td>https://www1.folha.uol.com.br/opiniao/2023/03/...</td>\n",
              "      <td>Iniciamos o ano de 2023 renovando nossa espera...</td>\n",
              "      <td>2023-03-01 21:00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Titulo            Veiculo  \\\n",
              "0   Líderes vão à COP28 sob calor recorde e com em...    Bloomberg Línea   \n",
              "0   Na COP28, Brasil e mais 115 países preveem tri...  Folha de S. Paulo   \n",
              "1   Petrobras aprova retomada de obras de refinari...  Folha de S. Paulo   \n",
              "2   Agenda verde entra no radar da Câmara às véspe...  Folha de S. Paulo   \n",
              "3                                    Quente, fervendo  Folha de S. Paulo   \n",
              "..                                                ...                ...   \n",
              "59  Mundo não deixará de precisar de petróleo, diz...  Folha de S. Paulo   \n",
              "60  De carro popular à Foz do Amazonas, projetos c...  Folha de S. Paulo   \n",
              "61  Térmicas a carvão no Sul pesam nas emissões de...  Folha de S. Paulo   \n",
              "62                               Semântica petroleira  Folha de S. Paulo   \n",
              "63  Petrobras tem compromisso com a transição ener...  Folha de S. Paulo   \n",
              "\n",
              "                                                  URL  \\\n",
              "0   https://bloomberglinea.com.br/esg/lideres-vao-...   \n",
              "0   https://www1.folha.uol.com.br/ambiente/2023/12...   \n",
              "1   https://www1.folha.uol.com.br/mercado/2023/11/...   \n",
              "2   https://www1.folha.uol.com.br/mercado/2023/11/...   \n",
              "3   https://www1.folha.uol.com.br/opiniao/2023/11/...   \n",
              "..                                                ...   \n",
              "59  https://www1.folha.uol.com.br/ambiente/2023/05...   \n",
              "60  https://www1.folha.uol.com.br/ambiente/2023/05...   \n",
              "61  https://www1.folha.uol.com.br/ambiente/2023/10...   \n",
              "62  https://www1.folha.uol.com.br/opiniao/2023/03/...   \n",
              "63  https://www1.folha.uol.com.br/opiniao/2023/03/...   \n",
              "\n",
              "                                               Artigo  \\\n",
              "0   Expectativa é de que encontro da ONU resulte e...   \n",
              "0   O presidente da COP28, conferência do clima da...   \n",
              "1   A Petrobras aprovou em seu novo plano estratég...   \n",
              "2   O presidente da Câmara dos Deputados, Arthur L...   \n",
              "3   No Sul, chuvas torrenciais assolam cidades e l...   \n",
              "..                                                ...   \n",
              "59  Primeiro diretor da Petrobras voltado para tra...   \n",
              "60  A discussão em torno da exploração de petróleo...   \n",
              "61  As térmicas a carvão foram os destaques em emi...   \n",
              "62  Imagine se em fevereiro do ano passado Vladimi...   \n",
              "63  Iniciamos o ano de 2023 renovando nossa espera...   \n",
              "\n",
              "                    Data_Publicacao  \n",
              "0   29 de Novembro, 2023 | 03:37 PM  \n",
              "0               2023-12-02 19:25:00  \n",
              "1               2023-11-27 20:55:00  \n",
              "2               2023-11-27 08:00:00  \n",
              "3               2023-11-25 22:00:00  \n",
              "..                              ...  \n",
              "59              2023-05-31 11:00:00  \n",
              "60              2023-05-29 19:51:00  \n",
              "61              2023-10-19 12:00:00  \n",
              "62              2023-03-09 21:00:00  \n",
              "63              2023-03-01 21:00:00  \n",
              "\n",
              "[65 rows x 5 columns]"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "the_walk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "GzR4DCGczEFq",
        "outputId": "2940540b-9a8f-4611-804f-93281d447ff3"
      },
      "outputs": [],
      "source": [
        "petrobras_talk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Itwy_B9H7BUY"
      },
      "outputs": [],
      "source": [
        "# Lista com os termos de interesse\n",
        "termos = ['carbono', 'co2', 'descarbonização', 'net zero']\n",
        "\n",
        "# Caminho do PDF a ser analisado\n",
        "caminho_pdf = 'https://api.mziq.com/mzfilemanager/v2/d/80f2e993-0a30-421a-9470-a4d5c8ad5e9f/b411dff3-201d-68fa-29bb-42b49a250f19?origin=2'\n",
        "\n",
        "# Nome da empresa a ser pesquisada no noticiário (em minúsculas)\n",
        "empresa = 'Itaú'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bradesco_walk = raspa_bloomberg(empresa, termos, 15)\n",
        "\n",
        "#bradesco_talk = analisa_relatorio(empresa, caminho_pdf, termos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Titulo</th>\n",
              "      <th>Veiculo</th>\n",
              "      <th>URL</th>\n",
              "      <th>Artigo</th>\n",
              "      <th>Data_Publicacao</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Titulo, Veiculo, URL, Artigo, Data_Publicacao]\n",
              "Index: []"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bradesco_walk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "def raspa_folha(empresa, termos):\n",
        "\n",
        "    # Cria um dataframe Pandas com 5 colunas\n",
        "    df = pd.DataFrame(columns=['Titulo', 'Veiculo', 'URL', 'Artigo', 'Data_Publicacao'])\n",
        "\n",
        "    nome_veiculo = 'Folha de S. Paulo'\n",
        "    links_folha = []\n",
        "\n",
        "    # ETAPA 1: Busca pelas matérias que contenham tanto o nome da empresa quanto os termos de interesse\n",
        "\n",
        "    for termo in termos:\n",
        "\n",
        "        # Monta a URL de busca a partir dos parâmetros\n",
        "        # Substitua todos os espaços em branco por '+'\n",
        "        busca = termo.replace(\" \", \"+\")\n",
        "\n",
        "        url = f'https://search.folha.uol.com.br/search?q={empresa}+{busca}&periodo=ano&sd=&ed=&site=todos'\n",
        "        print(url)\n",
        "\n",
        "        # requisita a primeira url de busca\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            print(f'{nome_veiculo} visitado com sucesso.')\n",
        "            sopa = BeautifulSoup(response.text, 'html.parser')\n",
        "            resultados = sopa.find('div', {'class' : 'c-search__result'}).get_text()\n",
        "            \n",
        "            # Encontre todos os números na string\n",
        "            num_resultados = re.findall('\\d+', resultados)\n",
        "\n",
        "            # O resultado é uma lista de strings, então precisamos converter o primeiro elemento para int\n",
        "            if len(num_resultados) > 0:\n",
        "                num_resultados = int(num_resultados[0])\n",
        "        \n",
        "                # Se houver resultados, calcula o número de páginas de resultados (cada página tem 25 resultados)\n",
        "                num_paginas = num_resultados // 25 + 1\n",
        "                print(f'A busca por {empresa} e {termo} retornou {num_resultados} artigos em {num_paginas} páginas.')\n",
        "\n",
        "                # Cria uma lista para receber as URLs de cada matéria na primeira página de busca (ou única página, se for o caso)\n",
        "                links = sopa.find_all('div', {'class' : 'c-headline__content'})\n",
        "                links = [link.find('a').get('href') for link in links if '#' not in link.find('a').get('href')]\n",
        "                print(f'Página 1 de {num_paginas} visitada, com {len(links)} links adicionados.')\n",
        "\n",
        "                # Se houver apenas uma página de resultados, adiciona os links à lista\n",
        "                if num_paginas == 1:\n",
        "                    links_folha = links_folha + links            \n",
        "\n",
        "                # Se houver mais de uma página de resultados, adiciona os links à lista e visita as páginas restantes\n",
        "                else:\n",
        "                    links_folha = links_folha + links\n",
        "                    pagina = 0\n",
        "                    while pagina < (num_paginas -1):\n",
        "                        pagina = pagina + 1\n",
        "                        print(f'Página {pagina + 1} de {num_paginas} visitada.')\n",
        "                        url = f'https://search.folha.uol.com.br/search?q={empresa}+{busca}&periodo=ano&sd=&ed=&site=todos&results_count={num_resultados}&sr={pagina * 25 + 1}'\n",
        "                        print(url)\n",
        "                        response = requests.get(url)\n",
        "                        sopa = BeautifulSoup(response.text, 'html.parser')\n",
        "                        links = []\n",
        "                        links = sopa.find_all('div', {'class' : 'c-headline__content'})\n",
        "                        links_folha.extend([link.find('a').get('href') for link in links if link.find('a').get('href') not in links_folha and '#' not in link.find('a').get('href')])\n",
        "            else:\n",
        "                print(f'A busca por {empresa} e {termo} não retornou resultados.')\n",
        "\n",
        "    len(links_folha)\n",
        "\n",
        "    # ETAPA 2: Visita cada link encontrado e salva o conteúdo dos artigos que contêm o nome da empresa e os termos de interesse\n",
        "\n",
        "    df = pd.DataFrame(columns=['Titulo', 'Veiculo', 'URL', 'Artigo', 'Data_Publicacao'])\n",
        "\n",
        "    for link in links_folha:\n",
        "        \n",
        "            # Requisita a página via requests\n",
        "            response = requests.get(link)\n",
        "        \n",
        "            # Verifica se a requisição foi bem-sucedida (código de status 200)\n",
        "            if response.status_code == 200:\n",
        "        \n",
        "                # Informa o status da requisição\n",
        "                print(f'{link} OK.')\n",
        "        \n",
        "                # Decodifica o conteúdo da resposta usando 'utf-8'\n",
        "                content = response.content.decode('utf-8')\n",
        "\n",
        "                # Cria um objeto BeautifulSoup com o conteúdo HTML da página\n",
        "                soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "                # Encontra o título principal\n",
        "                titulo = soup.find('h1', class_='c-content-head__title').get_text().strip()\n",
        "        \n",
        "                # Encontra a data de publicação\n",
        "\n",
        "                # Find the 'time' element with the specified class\n",
        "                time_element = soup.find('time', class_='c-more-options__published-date')\n",
        "\n",
        "                # Get the value of 'datetime' attribute\n",
        "                data = time_element['datetime']\n",
        "                    \n",
        "                # Encontra todos os parágrafos\n",
        "                paragrafos = soup.find('div', class_='c-news__body').find_all('p')\n",
        "\n",
        "                # Extrai o texto de cada parágrafo e junta tudo em uma única string\n",
        "                texto = ' '.join(paragrafo.get_text().strip() for paragrafo in paragrafos) \n",
        "\n",
        "                df.loc[len(df.index)] = [titulo, nome_veiculo, link, texto, data]\n",
        "                \n",
        "            else:\n",
        "                print(f'ERRO. A requisição a {link} retornou o status {response.status_code}.')\n",
        "        \n",
        "            time.sleep(3) # dorme por 3 segundos\n",
        "    \n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bloomberg Línea visitado com sucesso.\n",
            "https://bloomberglinea.com.br/esg/relatorio-da-onu-aponta-qual-e-a-ameaca-silenciosa-do-aquecimento-global/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/esg/lideres-vao-a-cop28-sob-calor-recorde-e-com-emissoes-de-carbono-dos-paises-em-alta/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/2023/09/19/cop28-o-que-e-a-conferencia-do-clima-e-por-que-ela-e-importante/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/esg/brasil-chega-a-cop28-com-delegacao-de-15-ministros-e-aposta-alta-na-lideranca-climatica/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/2023/11/28/blackrock-reforma-em-bancos-de-desenvolvimento-liberaria-us4-tri-para-transicao-verde/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/agro/vibra-expansao-do-milho-abre-caminho-para-a-aposta-da-empresa-em-etanol-diz-ceo/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/2023/11/16/desaceleracao-economica-pode-ser-pedra-no-caminho-da-reducao-de-carbono/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/2023/11/13/marcas-buscam-plastico-zero-mas-neutralizacao-e-desafio-na-agenda-esg/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/2023/11/13/brasil-avanca-na-oferta-de-titulos-verdes-com-rendimentos-em-discussao/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/2023/11/10/o-plano-da-europa-para-restaurar-a-natureza-que-desapareceu-no-continente/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/negocios/empresas-chinesas-avancam-no-brasil-e-ja-tem-30-do-mercado-de-eletricos/ OK.\n",
            "O nome da empresa Bradesco foi encontrado neste texto.\n",
            "Nenhum dos termos de interesse foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/2023/11/07/iphone-reduz-emissoes-mas-vendas-em-alta-podem-dificultar-metas-sustentaveis-da-apple/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/esg/burger-king-diz-que-demanda-de-hamburguer-a-base-de-planta-parou-de-crescer/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/esg/com-valuation-baixo-mineradoras-sao-oportunidade-de-investimento-diz-blackrock/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://bloomberglinea.com.br/esg/como-o-uruguai-planeja-erguer-um-complexo-de-us-16-bi-para-o-hidrogenio-verde/ OK.\n",
            "O nome da empresa Bradesco não foi encontrado neste texto.\n",
            "https://search.folha.uol.com.br/search?q=Bradesco+carbono&periodo=ano&sd=&ed=&site=todos\n",
            "Folha de S. Paulo visitado com sucesso.\n",
            "A busca por Bradesco e carbono retornou 9 artigos em 1 páginas.\n",
            "Página 1 de 1 visitada, com 9 links adicionados.\n",
            "https://search.folha.uol.com.br/search?q=Bradesco+co2&periodo=ano&sd=&ed=&site=todos\n",
            "Folha de S. Paulo visitado com sucesso.\n",
            "A busca por Bradesco e co2 retornou 0 artigos em 1 páginas.\n",
            "Página 1 de 1 visitada, com 0 links adicionados.\n",
            "https://search.folha.uol.com.br/search?q=Bradesco+descarbonização&periodo=ano&sd=&ed=&site=todos\n",
            "Folha de S. Paulo visitado com sucesso.\n",
            "A busca por Bradesco e descarbonização não retornou resultados.\n",
            "https://search.folha.uol.com.br/search?q=Bradesco+net+zero&periodo=ano&sd=&ed=&site=todos\n",
            "Folha de S. Paulo visitado com sucesso.\n",
            "A busca por Bradesco e net zero retornou 0 artigos em 1 páginas.\n",
            "Página 1 de 1 visitada, com 0 links adicionados.\n",
            "https://top-of-mind.folha.uol.com.br/2023/10/empatado-com-visa-nubank-e-top-cartao-de-credito-pela-1a-vez.shtml OK.\n",
            "https://www1.folha.uol.com.br/mercado/2023/09/processo-dos-eua-contra-google-comeca-a-ser-julgado-nesta-terca-entenda-o-que-esta-em-jogo.shtml OK.\n",
            "https://www1.folha.uol.com.br/mercado/2023/08/wework-que-chegou-a-valer-us-47-bi-coloca-em-duvida-continuidade-da-operacao.shtml OK.\n",
            "https://www1.folha.uol.com.br/mercado/2023/06/presidentes-de-itau-e-bradesco-defendem-reforma-tributaria.shtml OK.\n",
            "https://www1.folha.uol.com.br/mercado/2023/05/desmatamento-e-um-dos-principais-desafios-climaticos-do-pais-diz-febraban.shtml OK.\n",
            "https://www1.folha.uol.com.br/mercado/2023/04/marina-silva-defende-que-reforma-tributaria-considere-mecanismos-de-baixo-carbono.shtml OK.\n",
            "https://www1.folha.uol.com.br/folha-social-mais/2023/03/evento-reune-500-liderancas-para-discutir-agenda-2030-em-novo-hub-de-economia-verde.shtml OK.\n",
            "https://www1.folha.uol.com.br/mercado/2023/02/brasil-nao-pode-ser-so-a-fazenda-do-mundo-diz-mercadante-ao-assumir-bndes.shtml OK.\n",
            "https://www1.folha.uol.com.br/mercado/2023/01/brasil-retoma-multilateralismo-em-davos-em-forum-com-muitas-crises-e-poucas-solucoes.shtml OK.\n"
          ]
        }
      ],
      "source": [
        "empresa = 'Bradesco'\n",
        "termos = ['carbono', 'co2', 'descarbonização', 'net zero']\n",
        "\n",
        "bbdc_walk = pd.concat([raspa_bloomberg(empresa, termos, 15), raspa_folha(empresa, termos)])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
